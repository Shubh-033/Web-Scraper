# Web Scraper Application

A full-stack web scraping application built with Node.js, Express, React, and Vite. Extract data from any website and export it in multiple formats (JSON, Excel, CSV).

## ğŸ—ï¸ Project Structure

```
scraper/
â”œâ”€â”€ backend/          # Express.js API server
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ scraper.js     # Puppeteer & Cheerio scraping endpoints
â”‚   â”‚   â””â”€â”€ export.js      # Excel, JSON, CSV export endpoints
â”‚   â”œâ”€â”€ server.js     # Express server setup
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/         # React + Vite UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ› ï¸ Tech Stack

- **Backend**: Node.js, Express.js
- **Scraping**: Puppeteer (JS-heavy sites), Cheerio (static HTML)
- **Frontend**: React, Vite, Tailwind CSS
- **Export**: ExcelJS, CSV export
- **Database**: MongoDB (optional, for storing scraped data)

## ğŸš€ Quick Start

### Prerequisites
- Node.js (v16 or higher)
- npm or yarn

### Backend Setup

```bash
cd backend
npm install

# Create .env file
cp .env.example .env

# Start development server
npm run dev
```

Server runs on `http://localhost:5000`

### Frontend Setup

```bash
cd frontend
npm install

# Start development server
npm run dev
```

Frontend runs on `http://localhost:5173`

## ğŸ“ API Endpoints

### Scraping
- `POST /api/scraper/cheerio` - Scrape static HTML with CSS selectors
- `POST /api/scraper/puppeteer` - Scrape JavaScript-heavy sites

Request body:
```json
{
  "url": "https://example.com",
  "selectors": {
    "title": ".product-title",
    "price": ".price"
  }
}
```

### Export
- `POST /api/export/json` - Export to JSON
- `POST /api/export/excel` - Export to Excel (.xlsx)
- `POST /api/export/csv` - Export to CSV

Request body:
```json
{
  "data": [{ "title": "Item 1", "price": "$10" }],
  "filename": "export.json"
}
```

## ğŸ¯ Features

- âœ… Scrape any website with CSS selectors
- âœ… Support for static and JavaScript-heavy sites
- âœ… Export to JSON, Excel, and CSV
- âœ… Multiple selector support
- âœ… Clean, modern UI with Tailwind CSS
- âœ… Error handling and validation

## ğŸ“¦ Dependencies

### Backend
- `express` - Web framework
- `puppeteer` - Browser automation
- `cheerio` - HTML parsing
- `axios` - HTTP client
- `exceljs` - Excel file generation
- `cors` - CORS middleware
- `dotenv` - Environment variables

### Frontend
- `react` - UI library
- `vite` - Build tool
- `tailwindcss` - CSS framework
- `axios` - HTTP client

## ğŸ”§ Development

### Adding New Selectors
Edit the selector list in the ScraperForm component to add custom data extraction fields.

### Customizing Export Formats
Modify the `/backend/routes/export.js` file to add new export formats.

### Database Integration (Future)
To store scraped data in MongoDB, update `/backend/routes/scraper.js` to save data before returning.

## âš ï¸ Important Notes

1. **Rate Limiting**: Be respectful when scraping. Add delays between requests if scraping large amounts of data.
2. **Terms of Service**: Always check the website's ToS and robots.txt before scraping.
3. **Headless Browser**: Puppeteer downloads a browser on first install (~150MB).
4. **Selectors**: Test CSS selectors in browser DevTools before using them.

## ğŸ› Troubleshooting

### Puppeteer issues
```bash
# If Puppeteer fails to download Chromium on Windows
npm install --build-from-source
```

### CORS errors
Ensure `CORS_ORIGIN` in `.env` matches your frontend URL.

### Port already in use
Change `PORT` in backend `.env` or `port` in frontend `vite.config.js`.

## ğŸ“„ License

MIT
